<sect1 id="design" xreflabel="Web Robot Detection Design Documentation">
  <title>Web Robot Detection Design Documentation</title>
  <authorblurb>
    <para>
      By <ulink url="mailto:rogerh@arsdigita.com">Roger Hsueh</ulink>
    </para>
  </authorblurb>

  <sect2 id="design-essentials" xreflabel="Essentials">
    <title>Essentials</title>

    <itemizedlist>
      <listitem>
	<para>
	  ACS Administrator directory: 
	  <ulink url="/robot-detection/admin">/robot-detection/admin</ulink>
	</para>
      </listitem>

      <listitem><para>Tcl API 
	  <itemizedlist>
	    <listitem><para><ulink url="/api-doc/procs-file-view?path=%2fpackages%2frobot%2ddetection%2ftcl%2frobot%2ddetection%2dprocs%2etcl">
		  robot-detection-procs.tcl</ulink></para></listitem>

	    <listitem><para><ulink url="/api-doc/procs-file-view?path=%2fpackages%2frobot%2ddetection%2ftcl%2frobot%2ddetection%2dinit%2etcl">
		  robot-detection-init.tcl</ulink></para></listitem>
	  </itemizedlist>
	</para></listitem>

      <listitem><para>PL/SQL file 

	  <itemizedlist>
	    <listitem><para><ulink url="/doc/sql/display-sql?url=robot-detection-create.sql&amp;package_key=robot-detection">
		  robot-detection.create.sql</ulink></para></listitem>
	  </itemizedlist>
	</para></listitem>

      <listitem><para><xref linkend="requirements"></xref></para></listitem>
    </itemizedlist>
  </sect2>

  <sect2 id="design-introduction" xreflabel="Introduction">
    <title>Introduction</title>

    <para>The goal of this package is to expose some parts of the site
      that are normally not indexable by search engines. The package
      accomplishes this by performing the following functions:</para>

    <itemizedlist>
      <listitem><para>Storing and maintaining information about known robots on the
	  web (found at <ulink url="http://info.webcrawler.com/mak/projects/robots/active/all.txt">Web
	    Robots Database</ulink>) in a <computeroutput>robots</computeroutput> table.</para></listitem>

      <listitem><para>Using postauth ad_register_filters to identify robots
	  connecting to areas covered by robot detection by matching the
	  <computeroutput>UserAgent</computeroutput> header with known robots.</para></listitem>

      <listitem><para>Redirecting the robots to a special area on the site.</para></listitem>
    </itemizedlist>

    <para>
      This special area can either be a static page or a dynamic script
      that generates texts from the database which are suitable for
      search-engine indexing.
    </para>
  </sect2>

  <sect2 id="design-historical-considerations" xreflabel="Historical Considerations">
    <title>Historical Considerations</title>

    <para>Previous versions of the robot detection package relied on the
      AOLServer procedure <computeroutput>ns_register_filter</computeroutput>, which is
      deprecated as of ACS4. The robot detection package now uses the
      analogous <computeroutput>ad_register_filter</computeroutput> provided by the request
      processor in ACS 4.</para>

    <para>
      The queries are re-written to use the ACS 4 Database Access API.
      Other than that, this version is a pretty straightforward port.
    </para>
  </sect2>

  <sect2 id="design-competitive-analysis" xreflabel="Competitive Analysis">
    <title>Competitive Analysis</title>

    <para>On <ulink url="http://www.apache.org">Apache</ulink>, it's possible to
      accomplish more or less the same thing with <computeroutput><ulink url="http://httpd.apache.org/docs/mod/mod_rewrite.html">mod_rewrite</ulink></computeroutput>,
      but that's a low-level module that requires a lot of coding to make
      it work like the robot detection package.</para>
  </sect2>

  <sect2 id="design-design-tradeoffs" xreflabel="Design Tradeoffs">
    <title>Design Tradeoffs</title>

    <itemizedlist>
      <listitem><para>Only suitable for ACS administrator use.
	  What the web robots from search engines index depends on the
	  content in the the directory specified with the RedirectURL
	  parameter ("robot heaven"). Right now there's no automatic way to
	  generate the static (or pseudo static) files in "robot heaven." The
	  administrator should decide what sort of content to expose to
	  search engines and generate the files accordingly.
	</para></listitem>

      <listitem><para>To keep things simple, admin page permission and parameter
	  setting depends on the site-mapper. In particular, setting multiple
	  values for the FilterPattern parameter can be kind of clumsy.
	  Because ad_parameter only allows one value per key at this time, I
	  chose to use to use CSV (comma separated values) string to
	  represent a list of paths to be filtered. Fortunately, once
	  robot-detection is setup, it doesn't require further
	  administration.</para></listitem>

      <listitem><para>Refreshing the robots table entails dropping all rows from
	  robots table and then re-populating the data from scratch. This
	  ensures synchronization with the data from the web robots
	  database.</para></listitem>

      <listitem><para>The raw data comes from one static file that must be retrieved
	  in its entirety using http. However, building and maintaining real
	  rdbms backed registry of web robots is not worth the effort because
	  the number of robots is unlikely to grow very much.</para></listitem>
    </itemizedlist>

    <para></para>
    <para></para>
  </sect2>

  <sect2 id="design-api" xreflabel="API">
    <title>API</title>

    <para>The API for robot detection is quite simple: 
      (You can also use the <ulink url="/api-doc">API Browser</ulink> to view
      this)
    </para>

    <variablelist>
      <varlistentry><term><varname>ad_cache_robot_useragents</varname></term>
	<listitem>
	  <para>
	    Caches "User-Agent" values for known robots
	  </para>
	</listitem>
      </varlistentry>
      
      <varlistentry><term><varname>ad_replicate_web_robots_db</varname></term>
	<listitem>
	  <para>
	    Replicates data from the <ulink url="http://info.webcrawler.com/mak/projects/robots/active.html">Web
	      Robots Database</ulink> into a table in the database. The data is
	    published on the Web as a flat file, whose format is specified in
	    <ulink url="http://info.webcrawler.com/mak/projects/robots/active/schema.txt">http://info.webcrawler.com/mak/projects/robots/active/schema.txt</ulink>.
	    Basically, each non-blank line of the database corresponds to one
	    field (name-value pair) of a record that defines the
	    characteristics of a registered robot. Each record has a "robot-id"
	    field as a unique identifier. (There are many fields in the schema,
	    but, for now, the only ones we care about are: robot-id,
	    robot-name, robot-details-url, and robot-useragent.) Returns the
	    number of rows replicated. May raise a Tcl error that should be
	    caught by the caller.
	  </para>
	</listitem>
      </varlistentry>

      <varlistentry><term><varname>ad_robot_filter</varname> <emphasis>conn args why</emphasis></term>
	<listitem>
	  <para>
	    A filter to redirect any recognized robot to a specified page.
	  </para>
	  <para></para>
	</listitem>
      </varlistentry>

      <varlistentry><term><varname>ad_update_robot_list</varname></term>
	<listitem>
	  <para>
	    Will update the robots table if it is empty or if the number of
	    days since it was last updated is greater than the number of days
	    specified by the RefreshIntervalDays configuration parameter.
	  </para>
	</listitem>
      </varlistentry>

      <varlistentry><term><varname>robot_exists_p</varname> <emphasis>robot_id</emphasis></term>
	<listitem>
	  <para>
	    Returns 1 if a row already exists in the robots table with the
	    specified "robot_id", 0 otherwise.
	  </para>
	</listitem>
      </varlistentry>

      <varlistentry><term><varname>robot_p</varname> <emphasis>useragent</emphasis></term>
	<listitem>
	  <para>
	    Returns 1 if the useragent is recognized as a search engine, 0
	    otherwise.
	  </para>
	</listitem>
      </varlistentry>
    </variablelist>

  </sect2>
  <sect2 id="design-data-model-discussion" xreflabel="Data Model Discussion">
    <title>Data Model Discussion</title>

    <para>The data model has one table: <computeroutput>robots</computeroutput>. It mirrors a
      few critical fields of the schema laid out in 
      <ulink url="http://info.webcrawler.com/mak/projects/robots/active/schema.txt">Web
	Robots Database Schema</ulink>. Only <computeroutput>robot_id</computeroutput>,
      <computeroutput>robot_name</computeroutput>, <computeroutput>robot_details_url</computeroutput>,
      and <computeroutput>robot_useragent</computeroutput> are used in the current
      implementation.</para>

    <itemizedlist>
      <listitem><para><computeroutput>robot_id</computeroutput> is not generated from a sequence in
	  Oracle, rather it is the robot_id assigned by 
	  <ulink url="http://info.webcrawler.com/mak/projects/robots/active.html">Web
	    Robot Database</ulink> for each individual robot.</para></listitem>

      <listitem><para><computeroutput>robot_name</computeroutput> and 
	  <computeroutput>robot_details_url</computeroutput> are
	  used to display a list of robots in the <ulink url="/robot-detection/admin"></ulink> 
	  Robot Detection Administration page.
	</para></listitem>

      <listitem><para><computeroutput>robot_useragent</computeroutput> is the most important 
	  field of the bunch. An incoming http request is identified as a robot if
	  the <phrase>UserAgent</phrase> header matches of one of the
	  <computeroutput>robot_useragent</computeroutput> in the robots table.</para></listitem>
    </itemizedlist>

    <sect3 id="design-transactions" xreflabel="Transactions">
      <title>Transactions</title>
      <para>
	On server restarts, the robots table is refreshed with 
	<ulink url="http://info.webcrawler.com/mak/projects/robots/active/all.txt">the
	  raw data from Web Robots Database</ulink> if the table's age is beyond
	the number of days specified in the
	<computeroutput>RefreshIntervalDays</computeroutput> parameter. ACS Administrators can
	also refresh the robots table manually with 
	<ulink url="/robot-detection/admin">Robot Detection Administration</ulink>. 	
      </para>
    </sect3>
  </sect2>

  <sect2 id="design-user-interface" xreflabel="User Interface">
    <title>User Interface</title>

    <para>There is one page for ACS Administrators. It displays the
      current parameters and a list of robots, plus a link to manually
      refresh the robots table.</para>

  </sect2>
  <sect2 id="design-configurationparameters" xreflabel="Configuration/Parameters">
    <title>Configuration/Parameters</title>

    <informaltable><tgroup cols="3">
	<thead>
	  <row>
	    <entry align="center">Name</entry>
	    <entry align="center">Description</entry>
	    <entry align="center">Default</entry>
	  </row>
	</thead>

	<tbody>
	  <row>
	    <entry>WebRobotsDB</entry>
	    <entry>the URL of the Web Robots DB text file</entry>
	    <entry>
	      http://info.webcrawler.com/mak/projects/robots/active/all.txt</entry>
	  </row>

	  <row>
	    <entry>FilterPattern</entry>
	    <entry>a CSV string containing the URLs for ad_robot_filter to
	      checked</entry>
	    <entry>/members-only-stuff/*</entry>
	  </row>

	  <row>
	    <entry>RedirectURL</entry>
	    <entry>the URL where robots should be sent</entry>
	    <entry>/robot-heaven/</entry>
	  </row>

	  <row>
	    <entry>RefreshIntervalDays</entry>
	    <entry>How frequently (in days) the robots table should be refreshed
	      from the Web Robots DB</entry>
	    <entry>30</entry>
	  </row>
	</tbody></tgroup></informaltable>

    <para></para>
  </sect2>

  <sect2 id="design-future-improvementsareas-of-likely-change" xreflabel="Future Improvements/Areas of Likely Change">
    <title>Future Improvements/Areas of Likely Change</title>

    <para></para>

  </sect2>
  <sect2 id="design-authors" xreflabel="Authors">
    <title>Authors</title>

    <para></para>

    <itemizedlist>
      <listitem><para>System creator: <ulink url="mailto:michael@arsdigita.com">Michael
	    Yoon</ulink></para></listitem>

      <listitem><para>System owner: <ulink url="mailto:rogerh@arsdigita.com">Roger
	    Hsueh</ulink></para></listitem>

      <listitem><para>Documentation author: <ulink url="mailto:rogerh@arsdigita.com">Roger Hsueh</ulink></para></listitem>
    </itemizedlist>

    <para></para>
  </sect2>

  <sect2 id="design-revision-history" xreflabel="Revision History">
    <title>Revision History</title>

    <para></para>

    <informaltable><tgroup cols="4">
	<colspec colwidth="1*"></colspec>
	<colspec colwidth="5*"></colspec>
	<colspec colwidth="2*"></colspec>
	<colspec colwidth="2*"></colspec>
	<thead>
	  <row>
	    <entry align="center">
		Document Revision #</entry>
	    <entry align="center">
		Action Taken, Notes</entry>
	    <entry align="center">When?</entry>
	    <entry align="center">By Whom?</entry>
	  </row>
	</thead>

	<tbody>
	  <row>
	    <entry>0.3</entry>
	    <entry>Revised document based on comments 
              from Kevin Scaldeferri</entry>
	    <entry>2000-12-13</entry>
	    <entry>Roger Hsueh</entry>
	  </row>

	  <row>
	    <entry>0.2</entry>
	    <entry>Revised document based on comments 
              from Michael Bryzek</entry>
	    <entry>2000-12-07</entry>
	    <entry>Roger Hsueh</entry>
	  </row>

	  <row>
	    <entry>0.1</entry>
	    <entry>Create initial version</entry>
	    <entry>2000-12-06</entry>
	    <entry>Roger Hsueh</entry>
	  </row>
	</tbody></tgroup></informaltable>
  </sect2>
</sect1>


<!--
 Local Variables:
 sgml-parent-document: ("index.xml" "chapter" "sect1")
 End:
-->
